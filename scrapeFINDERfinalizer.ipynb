{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T00:11:51.686672Z",
     "start_time": "2025-06-08T00:11:50.195912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"https://docs.affinda.com/v2.0/docs/resume-job-description-parser\"\n",
    "#aff_ec31fbd945eadfec73eeb251b856bc814c9fc036\n",
    "#https://www.zenrows.com/blog/undetected-chromedriver#first-steps\n",
    "\n",
    "\"https://github.com/ismaelfi/Scrape-Linkedin-Posts\"\n",
    "\n"
   ],
   "id": "7ba4e5edd24e5cd6",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m clean_code = tokenize.untokenize(filtered)\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(output_path, \u001B[33m\"\u001B[39m\u001B[33mwb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     f.write(clean_code)\n",
      "\u001B[31mTypeError\u001B[39m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD:scrapeFINDERfinalizer.ipynb
     "end_time": "2025-06-08T00:27:45.502524Z",
     "start_time": "2025-06-08T00:27:44.767573Z"
=======
     "end_time": "2025-06-08T00:20:14.400611Z",
     "start_time": "2025-06-08T00:20:13.275115Z"
>>>>>>> 8de68b7ff93e1d79bac03c211af0dd2a6529a5ec:scrapeFINDER.ipynb
    }
   },
   "cell_type": "code",
   "source": [
    "import tokenize\n",
    "import io\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = r\"C:\\Users\\epw268\\Documents\\addinda_text_summ.txt\"\n",
    "output_path = r\"C:\\Users\\epw268\\Documents\\addinda_text_summNEW.txt\"  # To overwrite; change if separate file desired\n",
    "\n",
<<<<<<< HEAD:scrapeFINDERfinalizer.ipynb
    "triple_re = re.compile(r'^[urbfURBF]*(?:\"\"\"|\\'\\'\\')')\n",
    "\n",
    "# Read original file as text and prepare mask\n",
    "with open(input_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# Create mask for each character\n",
    "mask = [bytearray(b\"\\x01\" * (len(line))) for line in lines]\n",
    "\n",
    "# Tokenize and update mask: skip comments and triple-quoted strings\n",
    "with open(input_path, 'rb') as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    for tok in tqdm(tokens, desc=\"Scanning tokens\"):\n",
    "        toknum, tokval, start, end, _ = tok\n",
    "        if toknum == tokenize.COMMENT:\n",
    "            # Comment: remove from start column to end of line\n",
    "            line_idx, col = start\n",
    "            for i in range(col, len(lines[line_idx-1])):\n",
    "                mask[line_idx-1][i] = 0\n",
    "        elif toknum == tokenize.STRING and triple_re.match(tokval):\n",
    "            # Triple-quoted string: remove entire span\n",
    "            (s_line, s_col), (e_line, e_col) = start, end\n",
    "            for ln in range(s_line-1, e_line):\n",
    "                start_col = s_col if ln == s_line-1 else 0\n",
    "                end_col = e_col if ln == e_line-1 else len(lines[ln])\n",
    "                for i in range(start_col, end_col):\n",
    "                    mask[ln][i] = 0\n",
    "        # Skip encoding declarations, they are at start only\n",
    "\n",
    "# Reconstruct cleaned text\n",
    "cleaned_lines = []\n",
    "for ln, line in enumerate(lines):\n",
    "    cleaned_line = ''.join(ch for i, ch in enumerate(line) if mask[ln][i])\n",
    "    cleaned_lines.append(cleaned_line)\n",
    "\n",
    "# Write out cleaned text\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(cleaned_lines)"
=======
    "# Regex to detect triple-quoted strings (with possible prefixes)\n",
    "triple_re = re.compile(r'^[urbfURBF]*(?:\"\"\"|\\'\\'\\')')\n",
    "\n",
    "with open(input_path, \"rb\") as f:\n",
    "    tokens = tokenize.tokenize(f.readline)\n",
    "    filtered = []\n",
    "    for tok in tqdm(tokens, desc=\"Processing tokens\"):\n",
    "        toknum, tokval, start, end, line = tok\n",
    "        if toknum in (tokenize.COMMENT, tokenize.ENCODING):\n",
    "            continue\n",
    "        if toknum == tokenize.STRING and triple_re.match(tokval):\n",
    "            continue\n",
    "        filtered.append((toknum, tokval))\n",
    "\n",
    "clean_code = tokenize.untokenize(filtered)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_code)\n"
>>>>>>> 8de68b7ff93e1d79bac03c211af0dd2a6529a5ec:scrapeFINDER.ipynb
   ],
   "id": "4c861e0ba15a805",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:scrapeFINDERfinalizer.ipynb
      "Scanning tokens: 169392it [00:00, 307829.76it/s]\n"
     ]
    }
   ],
   "execution_count": 7
=======
      "Processing tokens: 169392it [00:01, 162284.46it/s]\n"
     ]
    }
   ],
   "execution_count": 4
>>>>>>> 8de68b7ff93e1d79bac03c211af0dd2a6529a5ec:scrapeFINDER.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class LinkedInJobSearch:\n",
    "    def __init__(self, access_token):\n",
    "        \"\"\"\n",
    "        Initialize the LinkedIn API client\n",
    "        \n",
    "        Args:\n",
    "            access_token: LinkedIn OAuth 2.0 access token\n",
    "        \"\"\"\n",
    "        self.access_token = access_token\n",
    "        self.base_url = \"https://api.linkedin.com/v2\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.access_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Restli-Protocol-Version\": \"2.0.0\"\n",
    "        }\n",
    "    \n",
    "    def get_greenhouse_job_posting_date(self, job_url):\n",
    "        \"\"\"\n",
    "        Extract the job posting date from a Greenhouse job URL\n",
    "        \n",
    "        Args:\n",
    "            job_url: The Greenhouse job URL\n",
    "            \n",
    "        Returns:\n",
    "            datetime object of the posting date or None if not found\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(job_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Look for posting date in various possible locations\n",
    "            # Greenhouse might store this in meta tags or specific divs\n",
    "            date_selectors = [\n",
    "                'meta[property=\"article:published_time\"]',\n",
    "                'meta[name=\"publish_date\"]',\n",
    "                'time[datetime]',\n",
    "                '.posting-date',\n",
    "                '.date-posted'\n",
    "            ]\n",
    "            \n",
    "            for selector in date_selectors:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    if element.name == 'meta':\n",
    "                        date_str = element.get('content')\n",
    "                    elif element.name == 'time':\n",
    "                        date_str = element.get('datetime')\n",
    "                    else:\n",
    "                        date_str = element.text.strip()\n",
    "                    \n",
    "                    if date_str:\n",
    "                        # Try to parse the date\n",
    "                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%B %d, %Y']:\n",
    "                            try:\n",
    "                                return datetime.strptime(date_str.split('T')[0], fmt)\n",
    "                            except:\n",
    "                                continue\n",
    "            \n",
    "            print(\"Warning: Could not find posting date on the page\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching job posting: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search_people_posts(self, company_name, keyword, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Search for LinkedIn posts from people at a specific company containing a keyword\n",
    "        \n",
    "        Args:\n",
    "            company_name: Name of the company (e.g., \"DoorDash\")\n",
    "            keyword: Keyword to search for in posts (e.g., \"Hiring\")\n",
    "            start_date: Start date for the search range\n",
    "            end_date: End date for the search range\n",
    "            \n",
    "        Returns:\n",
    "            List of posts matching the criteria\n",
    "        \"\"\"\n",
    "        # Note: LinkedIn's public API has limitations on search functionality\n",
    "        # You might need to use the LinkedIn Marketing API or Sales Navigator API\n",
    "        # for more advanced search capabilities\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # First, find the company ID\n",
    "        company_id = self.get_company_id(company_name)\n",
    "        if not company_id:\n",
    "            print(f\"Could not find company ID for {company_name}\")\n",
    "            return results\n",
    "        \n",
    "        # Search for posts\n",
    "        # This is a simplified example - actual implementation depends on API access level\n",
    "        search_url = f\"{self.base_url}/ugcPosts\"\n",
    "        \n",
    "        params = {\n",
    "            \"q\": \"authors\",\n",
    "            \"authors\": f\"List(urn:li:organization:{company_id})\",\n",
    "            \"sortBy\": \"CREATED\",\n",
    "            \"count\": 50\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            posts = response.json().get('elements', [])\n",
    "            \n",
    "            for post in posts:\n",
    "                # Check if post contains the keyword and is within date range\n",
    "                post_text = post.get('specificContent', {}).get('com.linkedin.ugc.ShareContent', {}).get('shareCommentary', {}).get('text', '')\n",
    "                created_time = post.get('created', {}).get('time', 0) / 1000  # Convert from milliseconds\n",
    "                post_date = datetime.fromtimestamp(created_time)\n",
    "                \n",
    "                if (keyword.lower() in post_text.lower() and \n",
    "                    start_date <= post_date <= end_date):\n",
    "                    \n",
    "                    # Get author information\n",
    "                    author_urn = post.get('author')\n",
    "                    author_info = self.get_author_info(author_urn)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'author': author_info,\n",
    "                        'post_text': post_text,\n",
    "                        'post_date': post_date,\n",
    "                        'post_url': f\"https://www.linkedin.com/feed/update/{post.get('id')}\"\n",
    "                    })\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error searching posts: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_company_id(self, company_name):\n",
    "        \"\"\"\n",
    "        Get LinkedIn company ID from company name\n",
    "        \n",
    "        Args:\n",
    "            company_name: Name of the company\n",
    "            \n",
    "        Returns:\n",
    "            Company ID or None if not found\n",
    "        \"\"\"\n",
    "        search_url = f\"{self.base_url}/organizationLookup\"\n",
    "        params = {\"q\": \"vanityName\", \"vanityName\": company_name.lower().replace(\" \", \"\")}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            if data.get('elements'):\n",
    "                return data['elements'][0].get('id')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding company: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_author_info(self, author_urn):\n",
    "        \"\"\"\n",
    "        Get author information from URN\n",
    "        \n",
    "        Args:\n",
    "            author_urn: LinkedIn URN of the author\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with author information\n",
    "        \"\"\"\n",
    "        # Extract ID from URN\n",
    "        author_id = author_urn.split(':')[-1]\n",
    "        \n",
    "        # Note: Getting full profile information requires additional permissions\n",
    "        # This is a simplified version\n",
    "        return {\n",
    "            'id': author_id,\n",
    "            'urn': author_urn,\n",
    "            'profile_url': f\"https://www.linkedin.com/in/{author_id}\"\n",
    "        }\n",
    "    \n",
    "    def search_hiring_posts_near_job_date(self, job_url, company_name=\"DoorDash\", keyword=\"Hiring\", days_range=30):\n",
    "        \"\"\"\n",
    "        Main function to search for hiring posts within a date range of a job posting\n",
    "        \n",
    "        Args:\n",
    "            job_url: Greenhouse job URL\n",
    "            company_name: Company to search for\n",
    "            keyword: Keyword to search in posts\n",
    "            days_range: Number of days before/after job posting to search\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant posts\n",
    "        \"\"\"\n",
    "        # Get job posting date\n",
    "        job_date = self.get_greenhouse_job_posting_date(job_url)\n",
    "        \n",
    "        if not job_date:\n",
    "            # If we can't find the date, use current date as fallback\n",
    "            print(\"Using current date as fallback\")\n",
    "            job_date = datetime.now()\n",
    "        \n",
    "        # Calculate date range\n",
    "        start_date = job_date - timedelta(days=days_range)\n",
    "        end_date = job_date + timedelta(days=days_range)\n",
    "        \n",
    "        print(f\"Searching for '{keyword}' posts from {company_name} employees\")\n",
    "        print(f\"Date range: {start_date.date()} to {end_date.date()}\")\n",
    "        \n",
    "        # Search for posts\n",
    "        results = self.search_people_posts(company_name, keyword, start_date, end_date)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # LinkedIn OAuth 2.0 access token\n",
    "    # You need to obtain this through LinkedIn's OAuth flow\n",
    "    # See: https://docs.microsoft.com/en-us/linkedin/shared/authentication/authorization-code-flow\n",
    "    ACCESS_TOKEN = os.environ.get('LINKEDIN_ACCESS_TOKEN', 'YOUR_ACCESS_TOKEN_HERE')\n",
    "    \n",
    "    if ACCESS_TOKEN == 'YOUR_ACCESS_TOKEN_HERE':\n",
    "        print(\"Please set your LinkedIn access token in the LINKEDIN_ACCESS_TOKEN environment variable\")\n",
    "        print(\"You can obtain an access token by following LinkedIn's OAuth 2.0 flow\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the search client\n",
    "    searcher = LinkedInJobSearch(ACCESS_TOKEN)\n",
    "    \n",
    "    # Job URL to analyze\n",
    "    job_url = \"https://job-boards.greenhouse.io/doordashusa/jobs/6330949\"\n",
    "    \n",
    "    # Search for hiring posts\n",
    "    results = searcher.search_hiring_posts_near_job_date(\n",
    "        job_url=job_url,\n",
    "        company_name=\"DoorDash\",\n",
    "        keyword=\"Hiring\",\n",
    "        days_range=30\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFound {len(results)} relevant posts:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, post in enumerate(results, 1):\n",
    "        print(f\"\\nPost {i}:\")\n",
    "        print(f\"Author: {post['author']['profile_url']}\")\n",
    "        print(f\"Date: {post['post_date'].strftime('%Y-%m-%d %H:%M')}\")\n",
    "        print(f\"Text: {post['post_text'][:200]}...\" if len(post['post_text']) > 200 else f\"Text: {post['post_text']}\")\n",
    "        print(f\"URL: {post['post_url']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, email=None, password=None):\n",
    "        \"\"\"\n",
    "        Initialize the LinkedIn scraper\n",
    "\n",
    "        Args:\n",
    "            email: LinkedIn email (optional - for logged in scraping)\n",
    "            password: LinkedIn password (optional - for logged in scraping)\n",
    "        \"\"\"\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "        self.logged_in = False\n",
    "\n",
    "    def setup_driver(self):\n",
    "        \"\"\"\n",
    "        Set up undetected Chrome driver to avoid detection\n",
    "        \"\"\"\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-gpu')\n",
    "\n",
    "        # Optional: run headless\n",
    "        # options.add_argument('--headless')\n",
    "\n",
    "        self.driver = uc.Chrome(options=options)\n",
    "        self.driver.implicitly_wait(10)\n",
    "\n",
    "    def random_delay(self, min_seconds=1, max_seconds=3):\n",
    "        \"\"\"Add random delay to mimic human behavior\"\"\"\n",
    "        time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "    def scroll_slowly(self, pixels=300):\n",
    "        \"\"\"Scroll slowly to mimic human behavior\"\"\"\n",
    "        self.driver.execute_script(f\"window.scrollBy(0, {pixels});\")\n",
    "        self.random_delay(0.5, 1)\n",
    "\n",
    "    def login(self):\n",
    "        \"\"\"\n",
    "        Login to LinkedIn if credentials are provided\n",
    "        \"\"\"\n",
    "        if not self.email or not self.password:\n",
    "            print(\"No credentials provided, continuing without login\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            self.driver.get(\"https://www.linkedin.com/login\")\n",
    "            self.random_delay(2, 4)\n",
    "\n",
    "            # Enter email\n",
    "            email_input = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"username\"))\n",
    "            )\n",
    "            email_input.send_keys(self.email)\n",
    "            self.random_delay(0.5, 1)\n",
    "\n",
    "            # Enter password\n",
    "            password_input = self.driver.find_element(By.ID, \"password\")\n",
    "            password_input.send_keys(self.password)\n",
    "            self.random_delay(0.5, 1)\n",
    "\n",
    "            # Click login button\n",
    "            login_button = self.driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "            login_button.click()\n",
    "\n",
    "            self.random_delay(3, 5)\n",
    "\n",
    "            # Check if login was successful\n",
    "            if \"feed\" in self.driver.current_url or \"mynetwork\" in self.driver.current_url:\n",
    "                print(\"Login successful\")\n",
    "                self.logged_in = True\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Login may have failed - check for CAPTCHA or 2FA\")\n",
    "                return False\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Login error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_greenhouse_job_date(self, job_url):\n",
    "        \"\"\"\n",
    "        Extract job posting date from Greenhouse URL\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.driver.get(job_url)\n",
    "            self.random_delay(2, 3)\n",
    "\n",
    "            # Look for date in various possible locations\n",
    "            date_patterns = [\n",
    "                r'Posted on (\\w+ \\d+, \\d{4})',\n",
    "                r'(\\d{4}-\\d{2}-\\d{2})',\n",
    "                r'(\\d{1,2}/\\d{1,2}/\\d{4})'\n",
    "            ]\n",
    "\n",
    "            page_source = self.driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Check meta tags\n",
    "            meta_date = soup.find('meta', {'property': 'article:published_time'})\n",
    "            if meta_date:\n",
    "                date_str = meta_date.get('content', '')\n",
    "                if date_str:\n",
    "                    return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "\n",
    "            # Search for date patterns in text\n",
    "            text_content = soup.get_text()\n",
    "            for pattern in date_patterns:\n",
    "                match = re.search(pattern, text_content)\n",
    "                if match:\n",
    "                    date_str = match.group(1)\n",
    "                    # Try to parse the date\n",
    "                    for fmt in ['%B %d, %Y', '%Y-%m-%d', '%m/%d/%Y']:\n",
    "                        try:\n",
    "                            return datetime.strptime(date_str, fmt)\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "            print(\"Could not find job posting date\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting job date: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_linkedin_posts(self, company_name, keyword, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Search LinkedIn for posts containing keyword from company employees\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Build search URL\n",
    "        search_query = f'\"{keyword}\" \"{company_name}\"'\n",
    "        encoded_query = search_query.replace(' ', '%20').replace('\"', '%22')\n",
    "\n",
    "        # Try different search approaches\n",
    "        search_urls = [\n",
    "            f\"https://www.linkedin.com/search/results/content/?keywords={encoded_query}&origin=GLOBAL_SEARCH_HEADER\",\n",
    "            f\"https://www.linkedin.com/search/results/all/?keywords={encoded_query}\"\n",
    "        ]\n",
    "\n",
    "        for search_url in search_urls:\n",
    "            try:\n",
    "                self.driver.get(search_url)\n",
    "                self.random_delay(3, 5)\n",
    "\n",
    "                # Check for login wall\n",
    "                if \"login\" in self.driver.current_url and not self.logged_in:\n",
    "                    print(\"Hit login wall - results will be limited\")\n",
    "                    continue\n",
    "\n",
    "                # Scroll to load more results\n",
    "                for _ in range(3):  # Scroll 3 times\n",
    "                    self.scroll_slowly(500)\n",
    "                    self.random_delay(1, 2)\n",
    "\n",
    "                # Parse results\n",
    "                posts = self.driver.find_elements(By.CSS_SELECTOR, \"[data-chameleon-result-urn]\")\n",
    "\n",
    "                for post in posts[:10]:  # Limit to first 10 to avoid detection\n",
    "                    try:\n",
    "                        # Extract post text\n",
    "                        text_element = post.find_element(By.CSS_SELECTOR, \".break-words\")\n",
    "                        post_text = text_element.text\n",
    "\n",
    "                        # Check if it contains our keyword and company\n",
    "                        if keyword.lower() in post_text.lower() and company_name.lower() in post_text.lower():\n",
    "                            # Try to extract author and date\n",
    "                            author_element = post.find_element(By.CSS_SELECTOR, \".app-aware-link span[dir='ltr']\")\n",
    "                            author_name = author_element.text if author_element else \"Unknown\"\n",
    "\n",
    "                            # Extract relative date (e.g., \"2d\", \"1w\")\n",
    "                            date_element = post.find_element(By.CSS_SELECTOR, \"time\")\n",
    "                            relative_date = date_element.text if date_element else \"Unknown\"\n",
    "\n",
    "                            # Try to get post URL\n",
    "                            link_element = post.find_element(By.CSS_SELECTOR, \"a[href*='/posts/']\")\n",
    "                            post_url = link_element.get_attribute('href') if link_element else \"\"\n",
    "\n",
    "                            results.append({\n",
    "                                'author': author_name,\n",
    "                                'text': post_text[:200] + \"...\" if len(post_text) > 200 else post_text,\n",
    "                                'relative_date': relative_date,\n",
    "                                'url': post_url\n",
    "                            })\n",
    "\n",
    "                    except NoSuchElementException:\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching posts: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def alternative_search_method(self, company_name, keyword):\n",
    "        \"\"\"\n",
    "        Alternative method: Search via Google with site:linkedin.com\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Use Google to search LinkedIn\n",
    "        search_query = f'site:linkedin.com/posts \"{keyword}\" \"{company_name}\" \"hiring\"'\n",
    "        google_url = f\"https://www.google.com/search?q={search_query.replace(' ', '+')}\"\n",
    "\n",
    "        try:\n",
    "            self.driver.get(google_url)\n",
    "            self.random_delay(2, 3)\n",
    "\n",
    "            # Get search results\n",
    "            search_results = self.driver.find_elements(By.CSS_SELECTOR, \"div.g\")\n",
    "\n",
    "            for result in search_results[:5]:  # Limit results\n",
    "                try:\n",
    "                    link = result.find_element(By.CSS_SELECTOR, \"a\")\n",
    "                    url = link.get_attribute('href')\n",
    "                    title = result.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "                    snippet = result.find_element(By.CSS_SELECTOR, \"span.aCOpRe\").text\n",
    "\n",
    "                    if \"linkedin.com\" in url and keyword.lower() in snippet.lower():\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'snippet': snippet,\n",
    "                            'url': url\n",
    "                        })\n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Google search: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run_search(self, job_url, company_name=\"DoorDash\", keyword=\"Hiring\"):\n",
    "        \"\"\"\n",
    "        Main method to run the search\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "\n",
    "            # Get job posting date\n",
    "            print(\"Fetching job posting date...\")\n",
    "            job_date = self.get_greenhouse_job_date(job_url)\n",
    "\n",
    "            if not job_date:\n",
    "                job_date = datetime.now()\n",
    "                print(\"Using current date as fallback\")\n",
    "\n",
    "            # Calculate date range\n",
    "            start_date = job_date - timedelta(days=30)\n",
    "            end_date = job_date + timedelta(days=30)\n",
    "\n",
    "            print(f\"\\nSearching for '{keyword}' posts from {company_name}\")\n",
    "            print(f\"Date range: {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "            # Try to login if credentials provided\n",
    "            if self.email and self.password:\n",
    "                print(\"\\nAttempting to login...\")\n",
    "                self.login()\n",
    "\n",
    "            # Method 1: Direct LinkedIn search\n",
    "            print(\"\\nSearching LinkedIn directly...\")\n",
    "            linkedin_results = self.search_linkedin_posts(company_name, keyword, start_date, end_date)\n",
    "\n",
    "            # Method 2: Google search\n",
    "            print(\"\\nSearching via Google...\")\n",
    "            google_results = self.alternative_search_method(company_name, keyword)\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"RESULTS FROM LINKEDIN SEARCH:\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            if linkedin_results:\n",
    "                for i, result in enumerate(linkedin_results, 1):\n",
    "                    print(f\"\\n{i}. {result['author']} ({result['relative_date']})\")\n",
    "                    print(f\"   {result['text']}\")\n",
    "                    print(f\"   URL: {result['url']}\")\n",
    "            else:\n",
    "                print(\"No results found on LinkedIn\")\n",
    "\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"RESULTS FROM GOOGLE SEARCH:\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "            if google_results:\n",
    "                for i, result in enumerate(google_results, 1):\n",
    "                    print(f\"\\n{i}. {result['title']}\")\n",
    "                    print(f\"   {result['snippet']}\")\n",
    "                    print(f\"   URL: {result['url']}\")\n",
    "            else:\n",
    "                print(\"No results found via Google\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Option 1: Without login (limited results)\n",
    "    scraper = LinkedInScraper()\n",
    "\n",
    "    # Option 2: With login (better results but risk of account restrictions)\n",
    "    # scraper = LinkedInScraper(email=\"your_email@example.com\", password=\"your_password\")\n",
    "\n",
    "    job_url = \"https://job-boards.greenhouse.io/doordashusa/jobs/6330949\"\n",
    "\n",
    "    print(\"Starting LinkedIn scraper...\")\n",
    "    print(\"Note: This approach has limitations:\")\n",
    "    print(\"- LinkedIn actively blocks scrapers\")\n",
    "    print(\"- You may encounter CAPTCHAs\")\n",
    "    print(\"- Results will be limited without login\")\n",
    "    print(\"- Logged-in scraping risks account restrictions\")\n",
    "    print(\"\\nProceed with caution!\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    scraper.run_search(job_url, company_name=\"DoorDash\", keyword=\"Hiring\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "ffb86ce1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
